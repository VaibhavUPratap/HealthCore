{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0f54b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well_ID</th>\n",
       "      <th>S. No.</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Block</th>\n",
       "      <th>Village</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Year</th>\n",
       "      <th>pH</th>\n",
       "      <th>...</th>\n",
       "      <th>PO4</th>\n",
       "      <th>TH</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Na</th>\n",
       "      <th>K</th>\n",
       "      <th>F</th>\n",
       "      <th>TDS</th>\n",
       "      <th>SiO2</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W254029084355301</td>\n",
       "      <td>1</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>Solan</td>\n",
       "      <td>Nallagarh</td>\n",
       "      <td>JAGATPUR</td>\n",
       "      <td>31.1594</td>\n",
       "      <td>76.6785</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.44</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>tabula-2019 water quality data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W251908084361501</td>\n",
       "      <td>2</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>Solan</td>\n",
       "      <td>Nallagarh</td>\n",
       "      <td>BARUNA</td>\n",
       "      <td>31.1540</td>\n",
       "      <td>76.6384</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.40</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>tabula-2019 water quality data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W310955076364001</td>\n",
       "      <td>3</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>Solan</td>\n",
       "      <td>Nallagarh</td>\n",
       "      <td>BHATOLI</td>\n",
       "      <td>31.1651</td>\n",
       "      <td>76.6082</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.40</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>tabula-2019 water quality data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W310143076392701</td>\n",
       "      <td>4</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>Solan</td>\n",
       "      <td>Nallagarh</td>\n",
       "      <td>MAGANPURA</td>\n",
       "      <td>31.0200</td>\n",
       "      <td>76.6500</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.12</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>tabula-2019 water quality data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W321645075471501</td>\n",
       "      <td>5</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>Kangra</td>\n",
       "      <td>Nurpur</td>\n",
       "      <td>PANJPIR</td>\n",
       "      <td>32.2800</td>\n",
       "      <td>75.7914</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.12</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>tabula-2019 water quality data.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Well_ID S. No.             State District      Block    Village  \\\n",
       "0  W254029084355301      1  Himachal Pradesh    Solan  Nallagarh   JAGATPUR   \n",
       "1  W251908084361501      2  Himachal Pradesh    Solan  Nallagarh     BARUNA   \n",
       "2  W310955076364001      3  Himachal Pradesh    Solan  Nallagarh    BHATOLI   \n",
       "3  W310143076392701      4  Himachal Pradesh    Solan  Nallagarh  MAGANPURA   \n",
       "4  W321645075471501      5  Himachal Pradesh   Kangra     Nurpur    PANJPIR   \n",
       "\n",
       "  Latitude Longitude  Year    pH  ...   PO4     TH    Ca   Mg    Na     K  \\\n",
       "0  31.1594   76.6785  2019  8.44  ...  <0.1   84.0  17.0   10  39.0   2.4   \n",
       "1  31.1540   76.6384  2019  8.40  ...  <0.1  116.0  10.0   18  23.0     1   \n",
       "2  31.1651   76.6082  2019  8.40  ...  <0.1  116.0  10.0   18  28.0   2.5   \n",
       "3  31.0200   76.6500  2019  8.12  ...  <0.1  200.0  38.0   26  68.0     4   \n",
       "4  32.2800   75.7914  2019  8.12  ...  <0.1  110.0  28.0  9.7  26.0  2.01   \n",
       "\n",
       "      F  TDS  SiO2                         source_file  \n",
       "0  0.20  NaN  11.0  tabula-2019 water quality data.csv  \n",
       "1  0.12  NaN  12.0  tabula-2019 water quality data.csv  \n",
       "2  0.15  NaN  13.0  tabula-2019 water quality data.csv  \n",
       "3  0.14  NaN  16.0  tabula-2019 water quality data.csv  \n",
       "4  0.22  NaN  30.0  tabula-2019 water quality data.csv  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('water_quality_master.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ad81d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Cases</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>Report_Date</th>\n",
       "      <th>Status</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BH/SUP/2019/01/01</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Supaul</td>\n",
       "      <td>Measles</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>02-01-19</td>\n",
       "      <td>06-01-19</td>\n",
       "      <td>outbreak. House</td>\n",
       "      <td>to house survey done. All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GJ/MHS/2019/01/05</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>Mahesana</td>\n",
       "      <td>Diarrheal</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>01-01-19</td>\n",
       "      <td>02-01-19</td>\n",
       "      <td>vomiting might</td>\n",
       "      <td>be due to consumption of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JH/GDA/2019/01/06</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>Godda</td>\n",
       "      <td>Chickenpox</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>08-01-19</td>\n",
       "      <td>08-01-19</td>\n",
       "      <td>confirmed as</td>\n",
       "      <td>Chicken Pox by the RRT team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KN/HAV/2019/01/09</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Haveri</td>\n",
       "      <td>Diarrheal</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>31-12-18</td>\n",
       "      <td>01-01-18</td>\n",
       "      <td>survey done.</td>\n",
       "      <td>2 Blood Samples sent to DPHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KN/HAV/2019/01/10</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Haveri</td>\n",
       "      <td>Diarrheal</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>03-01-19</td>\n",
       "      <td>04-01-19</td>\n",
       "      <td>to house</td>\n",
       "      <td>survey done. 2 Blood samples and 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unique_ID      State  District     Disease  Cases  Deaths  \\\n",
       "0  BH/SUP/2019/01/01      Bihar    Supaul     Measles      5       0   \n",
       "1  GJ/MHS/2019/01/05    Gujarat  Mahesana   Diarrheal     12       0   \n",
       "2  JH/GDA/2019/01/06  Jharkhand     Godda  Chickenpox     36       0   \n",
       "3  KN/HAV/2019/01/09  Karnataka    Haveri   Diarrheal     25       0   \n",
       "4  KN/HAV/2019/01/10  Karnataka    Haveri   Diarrheal     13       0   \n",
       "\n",
       "  Start_Date Report_Date           Status                            Comments  \n",
       "0   02-01-19    06-01-19  outbreak. House           to house survey done. All  \n",
       "1   01-01-19    02-01-19   vomiting might            be due to consumption of  \n",
       "2   08-01-19    08-01-19     confirmed as         Chicken Pox by the RRT team  \n",
       "3   31-12-18    01-01-18     survey done.        2 Blood Samples sent to DPHL  \n",
       "4   03-01-19    04-01-19         to house  survey done. 2 Blood samples and 1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"outbreak_master.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c8565e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 (Water Quality) columns:\n",
      "['Well_ID', 'S. No.', 'State', 'District', 'Block', 'Village', 'Latitude', 'Longitude', 'Year', 'pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2', 'source_file']\n",
      "\n",
      "df1 shape: (36032, 26)\n",
      "\n",
      "df1 info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36032 entries, 0 to 36031\n",
      "Data columns (total 26 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Well_ID      31039 non-null  object \n",
      " 1   S. No.       35337 non-null  object \n",
      " 2   State        34809 non-null  object \n",
      " 3   District     35220 non-null  object \n",
      " 4   Block        34622 non-null  object \n",
      " 5   Village      36013 non-null  object \n",
      " 6   Latitude     35941 non-null  object \n",
      " 7   Longitude    36022 non-null  object \n",
      " 8   Year         35958 non-null  object \n",
      " 9   pH           36028 non-null  object \n",
      " 10  EC in μS/cm  36028 non-null  object \n",
      " 11  CO3          36024 non-null  object \n",
      " 12  HCO3         36013 non-null  float64\n",
      " 13  Cl           36024 non-null  object \n",
      " 14  SO4          35644 non-null  object \n",
      " 15  NO3          31412 non-null  object \n",
      " 16  PO4          28142 non-null  object \n",
      " 17  TH           36026 non-null  float64\n",
      " 18  Ca           36027 non-null  float64\n",
      " 19  Mg           32478 non-null  object \n",
      " 20  Na           22427 non-null  object \n",
      " 21  K            22498 non-null  object \n",
      " 22  F            18096 non-null  object \n",
      " 23  TDS          17956 non-null  float64\n",
      " 24  SiO2         5983 non-null   object \n",
      " 25  source_file  36032 non-null  object \n",
      "dtypes: float64(4), object(22)\n",
      "memory usage: 7.1+ MB\n",
      "None\n",
      "\n",
      "==================================================\n",
      "\n",
      "df2 (Outbreak) columns:\n",
      "['Unique_ID', 'State', 'District', 'Disease', 'Cases', 'Deaths', 'Start_Date', 'Report_Date', 'Status', 'Comments']\n",
      "\n",
      "df2 shape: (2273, 10)\n",
      "\n",
      "df2 info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2273 entries, 0 to 2272\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unique_ID    2273 non-null   object\n",
      " 1   State        2273 non-null   object\n",
      " 2   District     2273 non-null   object\n",
      " 3   Disease      2273 non-null   object\n",
      " 4   Cases        2273 non-null   int64 \n",
      " 5   Deaths       2273 non-null   int64 \n",
      " 6   Start_Date   2273 non-null   object\n",
      " 7   Report_Date  2273 non-null   object\n",
      " 8   Status       2205 non-null   object\n",
      " 9   Comments     1931 non-null   object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 177.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of both dataframes\n",
    "print(\"df1 (Water Quality) columns:\")\n",
    "print(df1.columns.tolist())\n",
    "print(\"\\ndf1 shape:\", df1.shape)\n",
    "print(\"\\ndf1 info:\")\n",
    "print(df1.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"\\ndf2 (Outbreak) columns:\")\n",
    "print(df2.columns.tolist())\n",
    "print(\"\\ndf2 shape:\", df2.shape)\n",
    "print(\"\\ndf2 info:\")\n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3b3844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 first few rows with potential time/location info:\n",
      "              State District\n",
      "0  Himachal Pradesh    Solan\n",
      "1  Himachal Pradesh    Solan\n",
      "2  Himachal Pradesh    Solan\n",
      "3  Himachal Pradesh    Solan\n",
      "4  Himachal Pradesh   Kangra\n",
      "            Well_ID S. No.             State District      Block    Village  \\\n",
      "0  W254029084355301      1  Himachal Pradesh    Solan  Nallagarh   JAGATPUR   \n",
      "1  W251908084361501      2  Himachal Pradesh    Solan  Nallagarh     BARUNA   \n",
      "2  W310955076364001      3  Himachal Pradesh    Solan  Nallagarh    BHATOLI   \n",
      "3  W310143076392701      4  Himachal Pradesh    Solan  Nallagarh  MAGANPURA   \n",
      "4  W321645075471501      5  Himachal Pradesh   Kangra     Nurpur    PANJPIR   \n",
      "\n",
      "  Latitude Longitude  Year    pH  ...   PO4     TH    Ca   Mg    Na     K  \\\n",
      "0  31.1594   76.6785  2019  8.44  ...  <0.1   84.0  17.0   10  39.0   2.4   \n",
      "1  31.1540   76.6384  2019  8.40  ...  <0.1  116.0  10.0   18  23.0     1   \n",
      "2  31.1651   76.6082  2019  8.40  ...  <0.1  116.0  10.0   18  28.0   2.5   \n",
      "3  31.0200   76.6500  2019  8.12  ...  <0.1  200.0  38.0   26  68.0     4   \n",
      "4  32.2800   75.7914  2019  8.12  ...  <0.1  110.0  28.0  9.7  26.0  2.01   \n",
      "\n",
      "      F  TDS  SiO2                         source_file  \n",
      "0  0.20  NaN  11.0  tabula-2019 water quality data.csv  \n",
      "1  0.12  NaN  12.0  tabula-2019 water quality data.csv  \n",
      "2  0.15  NaN  13.0  tabula-2019 water quality data.csv  \n",
      "3  0.14  NaN  16.0  tabula-2019 water quality data.csv  \n",
      "4  0.22  NaN  30.0  tabula-2019 water quality data.csv  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "df2 first few rows with time/location info:\n",
      "       State  District Start_Date Report_Date\n",
      "0      Bihar    Supaul   02-01-19    06-01-19\n",
      "1    Gujarat  Mahesana   01-01-19    02-01-19\n",
      "2  Jharkhand     Godda   08-01-19    08-01-19\n",
      "3  Karnataka    Haveri   31-12-18    01-01-18\n",
      "4  Karnataka    Haveri   03-01-19    04-01-19\n",
      "\n",
      "Potential date columns in df1: ['Year']\n",
      "Date columns in df2: ['Start_Date', 'Report_Date']\n",
      "\n",
      "Unique states in df1: 11660\n",
      "Unique states in df2: 136\n",
      "\n",
      "Sample states in df1: ['Himachal Pradesh' 'Rajasthan' 'West Bengal' 'Haryana' 'A&N Islands']\n",
      "Sample states in df2: ['Bihar' 'Gujarat' 'Jharkhand' 'Karnataka' 'Tami']\n"
     ]
    }
   ],
   "source": [
    "# Check for potential time and location columns\n",
    "print(\"df1 first few rows with potential time/location info:\")\n",
    "print(df1[['State', 'District']].head() if 'State' in df1.columns else \"No State column in df1\")\n",
    "print(df1.head())\n",
    "\n",
    "print(\"\\ndf2 first few rows with time/location info:\")\n",
    "print(df2[['State', 'District', 'Start_Date', 'Report_Date']].head())\n",
    "\n",
    "# Check for date columns in df1\n",
    "date_cols_df1 = [col for col in df1.columns if 'date' in col.lower() or 'time' in col.lower() or 'year' in col.lower()]\n",
    "print(f\"\\nPotential date columns in df1: {date_cols_df1}\")\n",
    "\n",
    "date_cols_df2 = [col for col in df2.columns if 'date' in col.lower() or 'time' in col.lower() or 'year' in col.lower()]\n",
    "print(f\"Date columns in df2: {date_cols_df2}\")\n",
    "\n",
    "# Check unique states in both dataframes\n",
    "print(f\"\\nUnique states in df1: {df1['State'].nunique() if 'State' in df1.columns else 'No State column'}\")\n",
    "print(f\"Unique states in df2: {df2['State'].nunique()}\")\n",
    "\n",
    "print(f\"\\nSample states in df1: {df1['State'].unique()[:5] if 'State' in df1.columns else 'No State column'}\")\n",
    "print(f\"Sample states in df2: {df2['State'].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71e0d544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing df2 (outbreak data)...\n",
      "Sample of prepared df2:\n",
      "       State  District     Disease  Cases  Deaths Start_Date    Year  Month\n",
      "0      Bihar    Supaul     Measles      5       0 2019-01-02  2019.0    1.0\n",
      "1    Gujarat  Mahesana   Diarrheal     12       0 2019-01-01  2019.0    1.0\n",
      "2  Jharkhand     Godda  Chickenpox     36       0 2019-01-08  2019.0    1.0\n",
      "3  Karnataka    Haveri   Diarrheal     25       0 2018-12-31  2018.0   12.0\n",
      "4  Karnataka    Haveri   Diarrheal     13       0 2019-01-03  2019.0    1.0\n",
      "\n",
      "df1 columns: ['Well_ID', 'S. No.', 'State', 'District', 'Block', 'Village', 'Latitude', 'Longitude', 'Year', 'pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2', 'source_file']\n",
      "df1 sample data:\n",
      "            Well_ID S. No.             State District      Block    Village  \\\n",
      "0  W254029084355301      1  Himachal Pradesh    Solan  Nallagarh   JAGATPUR   \n",
      "1  W251908084361501      2  Himachal Pradesh    Solan  Nallagarh     BARUNA   \n",
      "2  W310955076364001      3  Himachal Pradesh    Solan  Nallagarh    BHATOLI   \n",
      "3  W310143076392701      4  Himachal Pradesh    Solan  Nallagarh  MAGANPURA   \n",
      "4  W321645075471501      5  Himachal Pradesh   Kangra     Nurpur    PANJPIR   \n",
      "\n",
      "  Latitude Longitude  Year    pH  ...   PO4     TH    Ca   Mg    Na     K  \\\n",
      "0  31.1594   76.6785  2019  8.44  ...  <0.1   84.0  17.0   10  39.0   2.4   \n",
      "1  31.1540   76.6384  2019  8.40  ...  <0.1  116.0  10.0   18  23.0     1   \n",
      "2  31.1651   76.6082  2019  8.40  ...  <0.1  116.0  10.0   18  28.0   2.5   \n",
      "3  31.0200   76.6500  2019  8.12  ...  <0.1  200.0  38.0   26  68.0     4   \n",
      "4  32.2800   75.7914  2019  8.12  ...  <0.1  110.0  28.0  9.7  26.0  2.01   \n",
      "\n",
      "      F  TDS  SiO2                         source_file  \n",
      "0  0.20  NaN  11.0  tabula-2019 water quality data.csv  \n",
      "1  0.12  NaN  12.0  tabula-2019 water quality data.csv  \n",
      "2  0.15  NaN  13.0  tabula-2019 water quality data.csv  \n",
      "3  0.14  NaN  16.0  tabula-2019 water quality data.csv  \n",
      "4  0.22  NaN  30.0  tabula-2019 water quality data.csv  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data Integration Strategy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# First, let's prepare df2 (outbreak data) for integration\n",
    "print(\"Preparing df2 (outbreak data)...\")\n",
    "df2_prep = df2.copy()\n",
    "\n",
    "# Convert date columns to datetime\n",
    "df2_prep['Start_Date'] = pd.to_datetime(df2_prep['Start_Date'], format='%d-%m-%y', errors='coerce')\n",
    "df2_prep['Report_Date'] = pd.to_datetime(df2_prep['Report_Date'], format='%d-%m-%y', errors='coerce')\n",
    "\n",
    "# Extract year and month for aggregation\n",
    "df2_prep['Year'] = df2_prep['Start_Date'].dt.year\n",
    "df2_prep['Month'] = df2_prep['Start_Date'].dt.month\n",
    "\n",
    "print(\"Sample of prepared df2:\")\n",
    "print(df2_prep[['State', 'District', 'Disease', 'Cases', 'Deaths', 'Start_Date', 'Year', 'Month']].head())\n",
    "\n",
    "# Check if df1 has time information\n",
    "print(f\"\\ndf1 columns: {df1.columns.tolist()}\")\n",
    "print(f\"df1 sample data:\")\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdd02ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting integration process...\n",
      "Step 1: Aggregating outbreak data...\n",
      "Aggregated outbreak data shape: (1472, 8)\n",
      "Sample aggregated outbreak data:\n",
      "      State   District    Year  Month  Total_Cases  Total_Deaths  \\\n",
      "0      Agar      Malwa  2023.0   11.0           10             0   \n",
      "1  Ambedkar      Nagar  2023.0    7.0           10             0   \n",
      "2  Anantnag  Hepatitis  2023.0    9.0           15             0   \n",
      "3       And    Nicobar  2019.0    6.0            8             0   \n",
      "4     Ashok      Nagar  2019.0   10.0           22             0   \n",
      "\n",
      "            Diseases  Outbreak_Count  \n",
      "0             Dengue               1  \n",
      "1         Chickenpox               1  \n",
      "2                  A               1  \n",
      "3  Andaman Diarrheal               1  \n",
      "4          Diarrheal               1  \n",
      "\n",
      "Step 2: Preparing water quality data...\n",
      "Sample prepared df1:\n",
      "            Well_ID S. No.             State District      Block    Village  \\\n",
      "0  W254029084355301      1  Himachal Pradesh    Solan  Nallagarh   JAGATPUR   \n",
      "1  W251908084361501      2  Himachal Pradesh    Solan  Nallagarh     BARUNA   \n",
      "2  W310955076364001      3  Himachal Pradesh    Solan  Nallagarh    BHATOLI   \n",
      "3  W310143076392701      4  Himachal Pradesh    Solan  Nallagarh  MAGANPURA   \n",
      "4  W321645075471501      5  Himachal Pradesh   Kangra     Nurpur    PANJPIR   \n",
      "\n",
      "  Latitude Longitude  Year    pH  ...   PO4     TH    Ca   Mg    Na     K  \\\n",
      "0  31.1594   76.6785  2019  8.44  ...  <0.1   84.0  17.0   10  39.0   2.4   \n",
      "1  31.1540   76.6384  2019  8.40  ...  <0.1  116.0  10.0   18  23.0     1   \n",
      "2  31.1651   76.6082  2019  8.40  ...  <0.1  116.0  10.0   18  28.0   2.5   \n",
      "3  31.0200   76.6500  2019  8.12  ...  <0.1  200.0  38.0   26  68.0     4   \n",
      "4  32.2800   75.7914  2019  8.12  ...  <0.1  110.0  28.0  9.7  26.0  2.01   \n",
      "\n",
      "      F  TDS  SiO2                         source_file  \n",
      "0  0.20  NaN  11.0  tabula-2019 water quality data.csv  \n",
      "1  0.12  NaN  12.0  tabula-2019 water quality data.csv  \n",
      "2  0.15  NaN  13.0  tabula-2019 water quality data.csv  \n",
      "3  0.14  NaN  16.0  tabula-2019 water quality data.csv  \n",
      "4  0.22  NaN  30.0  tabula-2019 water quality data.csv  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Integration of df1 (Water Quality) and df2 (Outbreak) data\n",
    "print(\"Starting integration process...\")\n",
    "\n",
    "# Step 1: Aggregate outbreak data by State, District, Year, and Month\n",
    "print(\"Step 1: Aggregating outbreak data...\")\n",
    "outbreak_agg = df2_prep.groupby(['State', 'District', 'Year', 'Month']).agg({\n",
    "    'Cases': 'sum',\n",
    "    'Deaths': 'sum',\n",
    "    'Disease': lambda x: ', '.join(x.unique()) if len(x.unique()) > 1 else x.iloc[0],\n",
    "    'Unique_ID': 'count'  # Count of outbreaks\n",
    "}).reset_index()\n",
    "\n",
    "outbreak_agg.rename(columns={\n",
    "    'Cases': 'Total_Cases',\n",
    "    'Deaths': 'Total_Deaths', \n",
    "    'Disease': 'Diseases',\n",
    "    'Unique_ID': 'Outbreak_Count'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"Aggregated outbreak data shape: {outbreak_agg.shape}\")\n",
    "print(\"Sample aggregated outbreak data:\")\n",
    "print(outbreak_agg.head())\n",
    "\n",
    "# Step 2: Prepare water quality data for integration\n",
    "print(\"\\nStep 2: Preparing water quality data...\")\n",
    "\n",
    "# Check if df1 has time columns, if not we'll need to create a strategy\n",
    "if 'Year' not in df1.columns:\n",
    "    # If df1 doesn't have explicit time columns, we'll create a cross-join approach\n",
    "    # or use a representative time period\n",
    "    print(\"df1 doesn't have explicit time columns. Creating time dimension...\")\n",
    "    \n",
    "    # Get unique years from outbreak data\n",
    "    unique_years = sorted(df2_prep['Year'].dropna().unique())\n",
    "    unique_months = list(range(1, 13))\n",
    "    \n",
    "    # Create a Cartesian product of df1 with time dimensions\n",
    "    df1_expanded = []\n",
    "    for year in unique_years:\n",
    "        for month in unique_months:\n",
    "            df1_temp = df1.copy()\n",
    "            df1_temp['Year'] = year\n",
    "            df1_temp['Month'] = month\n",
    "            df1_expanded.append(df1_temp)\n",
    "    \n",
    "    df1_prep = pd.concat(df1_expanded, ignore_index=True)\n",
    "    print(f\"Expanded df1 shape: {df1_prep.shape}\")\n",
    "else:\n",
    "    df1_prep = df1.copy()\n",
    "\n",
    "print(\"Sample prepared df1:\")\n",
    "print(df1_prep.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "393dea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Merging datasets...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Month'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2004\\2366436048.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 3: Perform the integration using merge\u001b[39;00m\n\u001b[32m      2\u001b[39m print(\u001b[33m\"Step 3: Merging datasets...\"\u001b[39m)\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Merge on State, District, Year, Month\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m integrated_df = pd.merge(\n\u001b[32m      6\u001b[39m     df1_prep,\n\u001b[32m      7\u001b[39m     outbreak_agg,\n\u001b[32m      8\u001b[39m     on=[\u001b[33m'State'\u001b[39m, \u001b[33m'District'\u001b[39m, \u001b[33m'Year'\u001b[39m, \u001b[33m'Month'\u001b[39m],\n",
      "\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Month'"
     ]
    }
   ],
   "source": [
    "# Step 3: Perform the integration using merge\n",
    "print(\"Step 3: Merging datasets...\")\n",
    "\n",
    "# Merge on State, District, Year, Month\n",
    "integrated_df = pd.merge(\n",
    "    df1_prep, \n",
    "    outbreak_agg, \n",
    "    on=['State', 'District', 'Year', 'Month'], \n",
    "    how='left'  # Keep all water quality records\n",
    ")\n",
    "\n",
    "# Fill NaN values for outbreak data (indicating no outbreaks for that location/time)\n",
    "outbreak_cols = ['Total_Cases', 'Total_Deaths', 'Outbreak_Count']\n",
    "for col in outbreak_cols:\n",
    "    integrated_df[col] = integrated_df[col].fillna(0)\n",
    "\n",
    "# For diseases, fill NaN with 'No Outbreak'\n",
    "integrated_df['Diseases'] = integrated_df['Diseases'].fillna('No Outbreak')\n",
    "\n",
    "print(f\"Integrated dataset shape: {integrated_df.shape}\")\n",
    "print(\"\\nIntegrated dataset columns:\")\n",
    "print(integrated_df.columns.tolist())\n",
    "\n",
    "print(\"\\nSample of integrated data:\")\n",
    "print(integrated_df.head(10))\n",
    "\n",
    "print(\"\\nIntegration summary:\")\n",
    "print(f\"Total records: {len(integrated_df)}\")\n",
    "print(f\"Records with outbreaks: {len(integrated_df[integrated_df['Total_Cases'] > 0])}\")\n",
    "print(f\"Records without outbreaks: {len(integrated_df[integrated_df['Total_Cases'] == 0])}\")\n",
    "print(f\"Unique states: {integrated_df['State'].nunique()}\")\n",
    "print(f\"Unique districts: {integrated_df['District'].nunique()}\")\n",
    "print(f\"Time range: {integrated_df['Year'].min()} - {integrated_df['Year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b195ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_prep columns: ['Well_ID', 'S. No.', 'State', 'District', 'Block', 'Village', 'Latitude', 'Longitude', 'Year', 'pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2', 'source_file']\n",
      "outbreak_agg columns: ['State', 'District', 'Year', 'Month', 'Total_Cases', 'Total_Deaths', 'Diseases', 'Outbreak_Count']\n",
      "Common columns: {'Year', 'State', 'District'}\n",
      "\n",
      "df1_prep sample:\n",
      "            Well_ID S. No.             State District      Block    Village  \\\n",
      "0  W254029084355301      1  Himachal Pradesh    Solan  Nallagarh   JAGATPUR   \n",
      "1  W251908084361501      2  Himachal Pradesh    Solan  Nallagarh     BARUNA   \n",
      "2  W310955076364001      3  Himachal Pradesh    Solan  Nallagarh    BHATOLI   \n",
      "3  W310143076392701      4  Himachal Pradesh    Solan  Nallagarh  MAGANPURA   \n",
      "4  W321645075471501      5  Himachal Pradesh   Kangra     Nurpur    PANJPIR   \n",
      "\n",
      "  Latitude Longitude  Year    pH  ...   PO4     TH    Ca   Mg    Na     K  \\\n",
      "0  31.1594   76.6785  2019  8.44  ...  <0.1   84.0  17.0   10  39.0   2.4   \n",
      "1  31.1540   76.6384  2019  8.40  ...  <0.1  116.0  10.0   18  23.0     1   \n",
      "2  31.1651   76.6082  2019  8.40  ...  <0.1  116.0  10.0   18  28.0   2.5   \n",
      "3  31.0200   76.6500  2019  8.12  ...  <0.1  200.0  38.0   26  68.0     4   \n",
      "4  32.2800   75.7914  2019  8.12  ...  <0.1  110.0  28.0  9.7  26.0  2.01   \n",
      "\n",
      "      F  TDS  SiO2                         source_file  \n",
      "0  0.20  NaN  11.0  tabula-2019 water quality data.csv  \n",
      "1  0.12  NaN  12.0  tabula-2019 water quality data.csv  \n",
      "2  0.15  NaN  13.0  tabula-2019 water quality data.csv  \n",
      "3  0.14  NaN  16.0  tabula-2019 water quality data.csv  \n",
      "4  0.22  NaN  30.0  tabula-2019 water quality data.csv  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "outbreak_agg sample:\n",
      "      State   District    Year  Month  Total_Cases  Total_Deaths  \\\n",
      "0      Agar      Malwa  2023.0   11.0           10             0   \n",
      "1  Ambedkar      Nagar  2023.0    7.0           10             0   \n",
      "2  Anantnag  Hepatitis  2023.0    9.0           15             0   \n",
      "3       And    Nicobar  2019.0    6.0            8             0   \n",
      "4     Ashok      Nagar  2019.0   10.0           22             0   \n",
      "\n",
      "            Diseases  Outbreak_Count  \n",
      "0             Dengue               1  \n",
      "1         Chickenpox               1  \n",
      "2                  A               1  \n",
      "3  Andaman Diarrheal               1  \n",
      "4          Diarrheal               1  \n"
     ]
    }
   ],
   "source": [
    "# Debug: Check actual columns in both datasets\n",
    "print(\"df1_prep columns:\", df1_prep.columns.tolist())\n",
    "print(\"outbreak_agg columns:\", outbreak_agg.columns.tolist())\n",
    "\n",
    "# Check for common columns\n",
    "common_cols = set(df1_prep.columns) & set(outbreak_agg.columns)\n",
    "print(f\"Common columns: {common_cols}\")\n",
    "\n",
    "# Check sample data\n",
    "print(\"\\ndf1_prep sample:\")\n",
    "print(df1_prep.head())\n",
    "print(\"\\noutbreak_agg sample:\")\n",
    "print(outbreak_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ca2d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Revised Integration based on available columns...\n",
      "Integration Option 1: Merge on State only\n",
      "State-only integration shape: (555209, 33)\n",
      "\n",
      "Integration Option 2: Merge on State and District\n",
      "State-District integration shape: (49036, 32)\n",
      "\n",
      "Final integrated dataset shape: (49036, 32)\n",
      "Final integrated dataset columns:\n",
      "['Well_ID', 'S. No.', 'State', 'District', 'Block', 'Village', 'Latitude', 'Longitude', 'Year_x', 'pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2', 'source_file', 'Year_y', 'Month', 'Total_Cases', 'Total_Deaths', 'Diseases', 'Outbreak_Count']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Revised Integration Strategy\n",
    "print(\"Step 3: Revised Integration based on available columns...\")\n",
    "\n",
    "# Since we can see the common columns, let's integrate based on State only\n",
    "# and optionally time if available in df1\n",
    "\n",
    "# Option 1: Merge on State only (most basic integration)\n",
    "print(\"Integration Option 1: Merge on State only\")\n",
    "integrated_df_state = pd.merge(\n",
    "    df1_prep, \n",
    "    outbreak_agg, \n",
    "    on=['State'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"State-only integration shape: {integrated_df_state.shape}\")\n",
    "\n",
    "# Option 2: If df1 has District column, merge on State and District\n",
    "if 'District' in df1_prep.columns:\n",
    "    print(\"\\nIntegration Option 2: Merge on State and District\")\n",
    "    integrated_df_district = pd.merge(\n",
    "        df1_prep, \n",
    "        outbreak_agg, \n",
    "        on=['State', 'District'], \n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"State-District integration shape: {integrated_df_district.shape}\")\n",
    "    # Use this as primary integration\n",
    "    integrated_df = integrated_df_district\n",
    "else:\n",
    "    print(\"\\nNo District column in df1, using State-only integration\")\n",
    "    integrated_df = integrated_df_state\n",
    "\n",
    "# Option 3: If both have time dimensions, add time-based merge\n",
    "if 'Year' in df1_prep.columns and 'Month' in df1_prep.columns:\n",
    "    print(\"\\nIntegration Option 3: Adding time dimension\")\n",
    "    if 'District' in df1_prep.columns:\n",
    "        integrated_df = pd.merge(\n",
    "            df1_prep, \n",
    "            outbreak_agg, \n",
    "            on=['State', 'District', 'Year', 'Month'], \n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        integrated_df = pd.merge(\n",
    "            df1_prep, \n",
    "            outbreak_agg, \n",
    "            on=['State', 'Year', 'Month'], \n",
    "            how='left'\n",
    "        )\n",
    "    print(f\"Full integration with time shape: {integrated_df.shape}\")\n",
    "\n",
    "print(f\"\\nFinal integrated dataset shape: {integrated_df.shape}\")\n",
    "print(\"Final integrated dataset columns:\")\n",
    "print(integrated_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f02e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Cleaning up the integrated dataset...\n",
      "Cleaned integrated dataset shape: (49036, 33)\n",
      "\n",
      "============================================================\n",
      "INTEGRATION SUMMARY\n",
      "============================================================\n",
      "✓ Successfully integrated water quality (df1) and outbreak (df2) data\n",
      "✓ Integration method: Merged on State and District\n",
      "✓ Time dimension: Added Year and Month from outbreak data\n",
      "✓ Final dataset shape: (49036, 33)\n",
      "\n",
      "Location coverage:\n",
      "  - States: 11660\n",
      "  - Districts: 13116\n",
      "  - Villages: 21980\n",
      "\n",
      "Time coverage:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTime coverage:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m integrated_df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Year range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[43mintegrated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(integrated_df[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m].max())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m integrated_df.columns:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Months: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(integrated_df[\u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m].dropna().unique())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\series.py:6518\u001b[39m, in \u001b[36mSeries.min\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m   6510\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m1\u001b[39m))\n\u001b[32m   6511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(\n\u001b[32m   6512\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   6516\u001b[39m     **kwargs,\n\u001b[32m   6517\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m6518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:12407\u001b[39m, in \u001b[36mNDFrame.min\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12400\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(\n\u001b[32m  12401\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12402\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12405\u001b[39m     **kwargs,\n\u001b[32m  12406\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m12407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12408\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  12409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12410\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12413\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12414\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:12396\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12392\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12394\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12398\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\series.py:6468\u001b[39m, in \u001b[36mSeries._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m   6463\u001b[39m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[32m   6464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   6465\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwith non-numeric dtypes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6467\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m6468\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\nanops.py:1098\u001b[39m, in \u001b[36m_nanminmax.<locals>.reduction\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _na_for_min_count(values, axis)\n\u001b[32m   1095\u001b[39m values, mask = _get_values(\n\u001b[32m   1096\u001b[39m     values, skipna, fill_value_typ=fill_value_typ, mask=mask\n\u001b[32m   1097\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1099\u001b[39m result = _maybe_null_out(result, axis, mask, values.shape)\n\u001b[32m   1100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\numpy\\_core\\_methods.py:48\u001b[39m, in \u001b[36m_amin\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     47\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: '<=' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# Step 4: Clean up and finalize the integrated dataset\n",
    "print(\"Step 4: Cleaning up the integrated dataset...\")\n",
    "\n",
    "# Handle NaN values in outbreak data\n",
    "outbreak_cols = ['Total_Cases', 'Total_Deaths', 'Outbreak_Count']\n",
    "for col in outbreak_cols:\n",
    "    integrated_df[col] = integrated_df[col].fillna(0)\n",
    "\n",
    "# Fill NaN diseases with 'No Outbreak'\n",
    "integrated_df['Diseases'] = integrated_df['Diseases'].fillna('No Outbreak')\n",
    "\n",
    "# Handle year columns (df1 had Year_x from water data, df2 had Year_y from outbreak data)\n",
    "if 'Year_x' in integrated_df.columns and 'Year_y' in integrated_df.columns:\n",
    "    # Use water quality year as primary, outbreak year as secondary\n",
    "    integrated_df['Water_Year'] = integrated_df['Year_x']\n",
    "    integrated_df['Outbreak_Year'] = integrated_df['Year_y'] \n",
    "    # Create a unified year column - prefer outbreak year when available, otherwise use water year\n",
    "    integrated_df['Year'] = integrated_df['Outbreak_Year'].fillna(integrated_df['Water_Year'])\n",
    "    # Drop the original year columns\n",
    "    integrated_df = integrated_df.drop(['Year_x', 'Year_y'], axis=1)\n",
    "\n",
    "print(f\"Cleaned integrated dataset shape: {integrated_df.shape}\")\n",
    "\n",
    "# Final summary and analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTEGRATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"✓ Successfully integrated water quality (df1) and outbreak (df2) data\")\n",
    "print(f\"✓ Integration method: Merged on State and District\")\n",
    "print(f\"✓ Time dimension: Added Year and Month from outbreak data\")\n",
    "print(f\"✓ Final dataset shape: {integrated_df.shape}\")\n",
    "\n",
    "print(f\"\\nLocation coverage:\")\n",
    "print(f\"  - States: {integrated_df['State'].nunique()}\")\n",
    "print(f\"  - Districts: {integrated_df['District'].nunique()}\")\n",
    "print(f\"  - Villages: {integrated_df['Village'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTime coverage:\")\n",
    "if 'Year' in integrated_df.columns:\n",
    "    print(f\"  - Year range: {int(integrated_df['Year'].min())} - {int(integrated_df['Year'].max())}\")\n",
    "if 'Month' in integrated_df.columns:\n",
    "    print(f\"  - Months: {sorted(integrated_df['Month'].dropna().unique())}\")\n",
    "\n",
    "print(f\"\\nOutbreak data coverage:\")\n",
    "print(f\"  - Records with outbreaks: {len(integrated_df[integrated_df['Total_Cases'] > 0])} ({len(integrated_df[integrated_df['Total_Cases'] > 0])/len(integrated_df)*100:.1f}%)\")\n",
    "print(f\"  - Records without outbreaks: {len(integrated_df[integrated_df['Total_Cases'] == 0])} ({len(integrated_df[integrated_df['Total_Cases'] == 0])/len(integrated_df)*100:.1f}%)\")\n",
    "print(f\"  - Total cases in dataset: {integrated_df['Total_Cases'].sum()}\")\n",
    "print(f\"  - Total deaths in dataset: {integrated_df['Total_Deaths'].sum()}\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(f\"  - Water quality features: {len([col for col in integrated_df.columns if col in ['pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2']])}\")\n",
    "print(f\"  - Location features: {len([col for col in integrated_df.columns if col in ['State', 'District', 'Block', 'Village', 'Latitude', 'Longitude']])}\")\n",
    "print(f\"  - Health outcome features: {len([col for col in integrated_df.columns if col in ['Total_Cases', 'Total_Deaths', 'Diseases', 'Outbreak_Count']])}\")\n",
    "\n",
    "print(\"\\nFirst few rows of integrated dataset:\")\n",
    "print(integrated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa82449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Cleaning up the integrated dataset...\n",
      "Cleaned integrated dataset shape: (49036, 33)\n",
      "\n",
      "============================================================\n",
      "INTEGRATION SUMMARY\n",
      "============================================================\n",
      "✓ Successfully integrated water quality (df1) and outbreak (df2) data\n",
      "✓ Integration method: Merged on State and District\n",
      "✓ Time dimension: Added Year and Month from outbreak data\n",
      "✓ Final dataset shape: (49036, 33)\n",
      "\n",
      "Location coverage:\n",
      "  - States: 11660\n",
      "  - Districts: 13116\n",
      "  - Villages: 21980\n",
      "\n",
      "Time coverage:\n",
      "  - Water_Year range: 0 - 3793\n",
      "  - Outbreak_Year range: 2009 - 2023\n",
      "  - Year range: 0 - 3793\n",
      "  - Months: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0), np.float64(11.0), np.float64(12.0)]\n",
      "\n",
      "Outbreak data coverage:\n",
      "  - Records with outbreaks: 16652 (34.0%)\n",
      "  - Records without outbreaks: 32384 (66.0%)\n",
      "  - Total cases in dataset: 671247.0\n",
      "  - Total deaths in dataset: 5419.0\n",
      "\n",
      "Key columns in integrated dataset:\n",
      "  - Water quality features: 16 (['pH', 'EC in μS/cm', 'CO3']...)\n",
      "  - Location features: 6 (['State', 'District', 'Block', 'Village', 'Latitude', 'Longitude'])\n",
      "  - Health outcome features: 4 (['Total_Cases', 'Total_Deaths', 'Diseases', 'Outbreak_Count'])\n",
      "\n",
      "Sample of integrated dataset:\n",
      "              State District    Village    pH  TDS  Total_Cases  Total_Deaths  \\\n",
      "0  Himachal Pradesh    Solan   JAGATPUR  8.44  NaN          0.0           0.0   \n",
      "1  Himachal Pradesh    Solan     BARUNA  8.40  NaN          0.0           0.0   \n",
      "2  Himachal Pradesh    Solan    BHATOLI  8.40  NaN          0.0           0.0   \n",
      "3  Himachal Pradesh    Solan  MAGANPURA  8.12  NaN          0.0           0.0   \n",
      "4  Himachal Pradesh   Kangra    PANJPIR  8.12  NaN          0.0           0.0   \n",
      "5  Himachal Pradesh   Kangra     JASSUR  8.45  NaN          0.0           0.0   \n",
      "6  Himachal Pradesh   Kangra     MOHTLI  8.18  NaN          0.0           0.0   \n",
      "7  Himachal Pradesh   Kangra      BANDH  8.45  NaN          0.0           0.0   \n",
      "8  Himachal Pradesh   Kangra     KANGRA  8.47  NaN          0.0           0.0   \n",
      "9  Himachal Pradesh   Kangra    TAKIPUR  8.25  NaN          0.0           0.0   \n",
      "\n",
      "      Diseases  \n",
      "0  No Outbreak  \n",
      "1  No Outbreak  \n",
      "2  No Outbreak  \n",
      "3  No Outbreak  \n",
      "4  No Outbreak  \n",
      "5  No Outbreak  \n",
      "6  No Outbreak  \n",
      "7  No Outbreak  \n",
      "8  No Outbreak  \n",
      "9  No Outbreak  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fixed - Clean up and finalize the integrated dataset\n",
    "print(\"Step 4: Cleaning up the integrated dataset...\")\n",
    "\n",
    "# Handle NaN values in outbreak data\n",
    "outbreak_cols = ['Total_Cases', 'Total_Deaths', 'Outbreak_Count']\n",
    "for col in outbreak_cols:\n",
    "    integrated_df[col] = integrated_df[col].fillna(0)\n",
    "\n",
    "# Fill NaN diseases with 'No Outbreak'\n",
    "integrated_df['Diseases'] = integrated_df['Diseases'].fillna('No Outbreak')\n",
    "\n",
    "print(f\"Cleaned integrated dataset shape: {integrated_df.shape}\")\n",
    "\n",
    "# Final summary and analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTEGRATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"✓ Successfully integrated water quality (df1) and outbreak (df2) data\")\n",
    "print(f\"✓ Integration method: Merged on State and District\")\n",
    "print(f\"✓ Time dimension: Added Year and Month from outbreak data\")\n",
    "print(f\"✓ Final dataset shape: {integrated_df.shape}\")\n",
    "\n",
    "print(f\"\\nLocation coverage:\")\n",
    "print(f\"  - States: {integrated_df['State'].nunique()}\")\n",
    "print(f\"  - Districts: {integrated_df['District'].nunique()}\")\n",
    "print(f\"  - Villages: {integrated_df['Village'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTime coverage:\")\n",
    "# Handle mixed data types in year columns\n",
    "year_cols = [col for col in integrated_df.columns if 'Year' in col]\n",
    "if year_cols:\n",
    "    for col in year_cols:\n",
    "        year_data = pd.to_numeric(integrated_df[col], errors='coerce').dropna()\n",
    "        if len(year_data) > 0:\n",
    "            print(f\"  - {col} range: {int(year_data.min())} - {int(year_data.max())}\")\n",
    "\n",
    "if 'Month' in integrated_df.columns:\n",
    "    month_data = pd.to_numeric(integrated_df['Month'], errors='coerce').dropna()\n",
    "    if len(month_data) > 0:\n",
    "        print(f\"  - Months: {sorted(month_data.unique())}\")\n",
    "\n",
    "print(f\"\\nOutbreak data coverage:\")\n",
    "print(f\"  - Records with outbreaks: {len(integrated_df[integrated_df['Total_Cases'] > 0])} ({len(integrated_df[integrated_df['Total_Cases'] > 0])/len(integrated_df)*100:.1f}%)\")\n",
    "print(f\"  - Records without outbreaks: {len(integrated_df[integrated_df['Total_Cases'] == 0])} ({len(integrated_df[integrated_df['Total_Cases'] == 0])/len(integrated_df)*100:.1f}%)\")\n",
    "print(f\"  - Total cases in dataset: {integrated_df['Total_Cases'].sum()}\")\n",
    "print(f\"  - Total deaths in dataset: {integrated_df['Total_Deaths'].sum()}\")\n",
    "\n",
    "print(f\"\\nKey columns in integrated dataset:\")\n",
    "water_quality_cols = ['pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2']\n",
    "location_cols = ['State', 'District', 'Block', 'Village', 'Latitude', 'Longitude']\n",
    "health_cols = ['Total_Cases', 'Total_Deaths', 'Diseases', 'Outbreak_Count']\n",
    "\n",
    "present_water_cols = [col for col in water_quality_cols if col in integrated_df.columns]\n",
    "present_location_cols = [col for col in location_cols if col in integrated_df.columns]\n",
    "present_health_cols = [col for col in health_cols if col in integrated_df.columns]\n",
    "\n",
    "print(f\"  - Water quality features: {len(present_water_cols)} ({present_water_cols[:3]}...)\")\n",
    "print(f\"  - Location features: {len(present_location_cols)} ({present_location_cols})\")\n",
    "print(f\"  - Health outcome features: {len(present_health_cols)} ({present_health_cols})\")\n",
    "\n",
    "print(\"\\nSample of integrated dataset:\")\n",
    "display_cols = ['State', 'District', 'Village', 'pH', 'TDS', 'Total_Cases', 'Total_Deaths', 'Diseases']\n",
    "available_display_cols = [col for col in display_cols if col in integrated_df.columns]\n",
    "print(integrated_df[available_display_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce3ea31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Saving the integrated dataset...\n",
      "✓ Integrated dataset saved as 'integrated_water_health_dataset.csv'\n",
      "\n",
      "WATER QUALITY & HEALTH OUTBREAK DATA INTEGRATION REPORT\n",
      "========================================================\n",
      "\n",
      "Dataset Information:\n",
      "- Source 1: Water Quality Data (df1) - 36032 records\n",
      "- Source 2: Health Outbreak Data (df2) - 2273 records  \n",
      "- Integrated Dataset: 49036 records, 33 features\n",
      "\n",
      "Integration Method:\n",
      "- Primary join: State + District\n",
      "- Time dimension: Year and Month from outbreak data\n",
      "- Missing outbreak data filled with zeros (indicating no outbreaks)\n",
      "\n",
      "Key Features Available:\n",
      "- Geographic: State, District, Block, Village, Latitude, Longitude\n",
      "- Water Quality: pH, TDS, EC, various chemical parameters\n",
      "- Health Outcomes: Total Cases, Total Deaths, Disease Types, Outbreak Count\n",
      "- Temporal: Year, Month information\n",
      "\n",
      "Potential Use Cases:\n",
      "1. Correlation analysis between water quality and health outbreaks\n",
      "2. Predictive modeling for outbreak risk based on water parameters\n",
      "3. Geographic clustering of health risks\n",
      "4. Temporal analysis of outbreak patterns\n",
      "5. Policy recommendations for water quality monitoring\n",
      "\n",
      "Next Steps:\n",
      "1. Exploratory Data Analysis (EDA)\n",
      "2. Feature engineering and selection\n",
      "3. Statistical correlation analysis\n",
      "4. Machine learning model development\n",
      "5. Visualization and reporting\n",
      "\n",
      "File saved: integrated_water_health_dataset.csv\n",
      "\n",
      "Data Quality Assessment:\n",
      "- Missing values per column:\n",
      "               Missing_Count  Missing_Percentage\n",
      "SiO2                   41812           85.267966\n",
      "Outbreak_Year          32384           66.041276\n",
      "Month                  32384           66.041276\n",
      "TDS                    18082           36.874949\n",
      "F                      17950           36.605759\n",
      "PO4                    17705           36.106126\n",
      "Na                     13605           27.744922\n",
      "K                      13534           27.600131\n",
      "Well_ID                 8734           17.811404\n",
      "NO3                     4620            9.421649\n",
      "\n",
      "✅ Integration complete! The dataset is ready for analysis and modeling.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save the integrated dataset\n",
    "print(\"Step 5: Saving the integrated dataset...\")\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'integrated_water_health_dataset.csv'\n",
    "integrated_df.to_csv(output_filename, index=False)\n",
    "print(f\"✓ Integrated dataset saved as '{output_filename}'\")\n",
    "\n",
    "# Create a summary report\n",
    "summary_report = f\"\"\"\n",
    "WATER QUALITY & HEALTH OUTBREAK DATA INTEGRATION REPORT\n",
    "========================================================\n",
    "\n",
    "Dataset Information:\n",
    "- Source 1: Water Quality Data (df1) - {df1.shape[0]} records\n",
    "- Source 2: Health Outbreak Data (df2) - {df2.shape[0]} records  \n",
    "- Integrated Dataset: {integrated_df.shape[0]} records, {integrated_df.shape[1]} features\n",
    "\n",
    "Integration Method:\n",
    "- Primary join: State + District\n",
    "- Time dimension: Year and Month from outbreak data\n",
    "- Missing outbreak data filled with zeros (indicating no outbreaks)\n",
    "\n",
    "Key Features Available:\n",
    "- Geographic: State, District, Block, Village, Latitude, Longitude\n",
    "- Water Quality: pH, TDS, EC, various chemical parameters\n",
    "- Health Outcomes: Total Cases, Total Deaths, Disease Types, Outbreak Count\n",
    "- Temporal: Year, Month information\n",
    "\n",
    "Potential Use Cases:\n",
    "1. Correlation analysis between water quality and health outbreaks\n",
    "2. Predictive modeling for outbreak risk based on water parameters\n",
    "3. Geographic clustering of health risks\n",
    "4. Temporal analysis of outbreak patterns\n",
    "5. Policy recommendations for water quality monitoring\n",
    "\n",
    "Next Steps:\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Feature engineering and selection\n",
    "3. Statistical correlation analysis\n",
    "4. Machine learning model development\n",
    "5. Visualization and reporting\n",
    "\n",
    "File saved: {output_filename}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Optional: Show data quality assessment\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(f\"- Missing values per column:\")\n",
    "missing_data = integrated_df.isnull().sum()\n",
    "missing_data_pct = (missing_data / len(integrated_df)) * 100\n",
    "quality_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_data_pct\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(quality_df[quality_df['Missing_Count'] > 0].head(10))\n",
    "\n",
    "print(f\"\\n✅ Integration complete! The dataset is ready for analysis and modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a608469d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>pH_mean</th>\n",
       "      <th>pH_std</th>\n",
       "      <th>pH_count</th>\n",
       "      <th>TDS_mean</th>\n",
       "      <th>TDS_std</th>\n",
       "      <th>TDS_count</th>\n",
       "      <th>F_mean</th>\n",
       "      <th>F_std</th>\n",
       "      <th>F_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Cases_count</th>\n",
       "      <th>Deaths_sum</th>\n",
       "      <th>Deaths_mean</th>\n",
       "      <th>Disease_count</th>\n",
       "      <th>Mortality_Rate</th>\n",
       "      <th>Water_Disease_Cases</th>\n",
       "      <th>Water_Disease_Deaths</th>\n",
       "      <th>Water_Disease_Outbreaks</th>\n",
       "      <th>Water_Disease_Rate</th>\n",
       "      <th>Cases_per_1000_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assam</td>\n",
       "      <td>7.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>265</td>\n",
       "      <td>181.72</td>\n",
       "      <td>131.33</td>\n",
       "      <td>265</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>115</td>\n",
       "      <td>199</td>\n",
       "      <td>1.73</td>\n",
       "      <td>115</td>\n",
       "      <td>5.532388</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.251599</td>\n",
       "      <td>3.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Badgam</td>\n",
       "      <td>586.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>363.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bihar</td>\n",
       "      <td>7.94</td>\n",
       "      <td>0.40</td>\n",
       "      <td>640</td>\n",
       "      <td>464.61</td>\n",
       "      <td>261.64</td>\n",
       "      <td>640</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>91</td>\n",
       "      <td>0.890648</td>\n",
       "      <td>355.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.565562</td>\n",
       "      <td>2.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>109.44</td>\n",
       "      <td>199.80</td>\n",
       "      <td>13</td>\n",
       "      <td>239.40</td>\n",
       "      <td>97.79</td>\n",
       "      <td>10</td>\n",
       "      <td>4.47</td>\n",
       "      <td>8.11</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.38</td>\n",
       "      <td>915</td>\n",
       "      <td>338.03</td>\n",
       "      <td>219.77</td>\n",
       "      <td>915</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>915</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>92</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>2272.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.352869</td>\n",
       "      <td>3.276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          State  pH_mean  pH_std  pH_count  TDS_mean  TDS_std  TDS_count  \\\n",
       "0         Assam     7.68    0.59       265    181.72   131.33        265   \n",
       "1        Badgam   586.00     NaN         1    363.32      NaN          1   \n",
       "2         Bihar     7.94    0.40       640    464.61   261.64        640   \n",
       "3    Chandigarh   109.44  199.80        13    239.40    97.79         10   \n",
       "4  Chhattisgarh     7.64    0.38       915    338.03   219.77        915   \n",
       "\n",
       "   F_mean  F_std  F_count  ...  Cases_count  Deaths_sum  Deaths_mean  \\\n",
       "0    0.44   0.46      265  ...          115         199         1.73   \n",
       "1     NaN    NaN        0  ...            2           1         0.50   \n",
       "2    0.37   0.38      640  ...           91          18         0.20   \n",
       "3    4.47   8.11       13  ...            3           0         0.00   \n",
       "4    0.63   0.56      915  ...           92          10         0.11   \n",
       "\n",
       "   Disease_count  Mortality_Rate  Water_Disease_Cases  Water_Disease_Deaths  \\\n",
       "0            115        5.532388               1268.0                   1.0   \n",
       "1              2        2.127660                  0.0                   0.0   \n",
       "2             91        0.890648                355.0                   7.0   \n",
       "3              3        0.000000                 27.0                   0.0   \n",
       "4             92        0.305250               2272.0                   6.0   \n",
       "\n",
       "   Water_Disease_Outbreaks  Water_Disease_Rate  Cases_per_1000_pop  \n",
       "0                     34.0           35.251599               3.597  \n",
       "1                      0.0            0.000000               0.047  \n",
       "2                     19.0           17.565562               2.021  \n",
       "3                      2.0           67.500000               0.040  \n",
       "4                     59.0           69.352869               3.276  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('integrated_health_water_dataset.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ebf2c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model training pipeline...\n",
      "Available columns in integrated dataset:\n",
      "['Well_ID', 'S. No.', 'State', 'District', 'Block', 'Village', 'Latitude', 'Longitude', 'pH', 'EC in μS/cm', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'PO4', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'SiO2', 'source_file', 'Month', 'Total_Cases', 'Total_Deaths', 'Diseases', 'Outbreak_Count', 'Water_Year', 'Outbreak_Year', 'Year']\n",
      "✓ pH column found\n",
      "❌ No temperature columns found\n",
      "✓ Cases column found (Total_Cases)\n",
      "\n",
      "Identified features: ['pH', 'Total_Cases']\n",
      "\n",
      "Dataset shape: (49036, 33)\n",
      "Non-null counts for identified features:\n",
      "  pH: 49032 non-null values\n",
      "  Total_Cases: 49036 non-null values\n"
     ]
    }
   ],
   "source": [
    "# Model Training: Temperature, pH, Cases -> Risk Level\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Setting up model training pipeline...\")\n",
    "\n",
    "# First, let's examine our integrated dataset for the required features\n",
    "print(\"Available columns in integrated dataset:\")\n",
    "print(integrated_df.columns.tolist())\n",
    "\n",
    "# Check if we have the required features (temperature, pH, cases)\n",
    "required_features = []\n",
    "target_col = None\n",
    "\n",
    "# Map available columns to our requirements\n",
    "if 'pH' in integrated_df.columns:\n",
    "    required_features.append('pH')\n",
    "    print(\"✓ pH column found\")\n",
    "else:\n",
    "    print(\"❌ pH column not found\")\n",
    "\n",
    "# Look for temperature-related columns\n",
    "temp_cols = [col for col in integrated_df.columns if 'temp' in col.lower() or 'temperature' in col.lower()]\n",
    "if temp_cols:\n",
    "    required_features.extend(temp_cols)\n",
    "    print(f\"✓ Temperature columns found: {temp_cols}\")\n",
    "else:\n",
    "    print(\"❌ No temperature columns found\")\n",
    "\n",
    "# Use Total_Cases as our cases feature\n",
    "if 'Total_Cases' in integrated_df.columns:\n",
    "    required_features.append('Total_Cases')\n",
    "    print(\"✓ Cases column found (Total_Cases)\")\n",
    "else:\n",
    "    print(\"❌ Cases column not found\")\n",
    "\n",
    "print(f\"\\nIdentified features: {required_features}\")\n",
    "\n",
    "# Check data availability\n",
    "print(f\"\\nDataset shape: {integrated_df.shape}\")\n",
    "print(f\"Non-null counts for identified features:\")\n",
    "for feature in required_features:\n",
    "    if feature in integrated_df.columns:\n",
    "        non_null_count = integrated_df[feature].notna().sum()\n",
    "        print(f\"  {feature}: {non_null_count} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e5993f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Risk Level target variable...\n",
      "Added TDS to input features\n",
      "Added F to input features\n",
      "Added NO3 to input features\n",
      "Added Cl to input features\n",
      "Added EC in μS/cm to input features\n",
      "Final input features: ['pH', 'Total_Cases', 'TDS', 'F', 'NO3', 'Cl', 'EC in μS/cm']\n",
      "Calculating risk levels...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Apply risk calculation\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalculating risk levels...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m model_data[\u001b[33m'\u001b[39m\u001b[33mRisk_Level\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mmodel_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_risk_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Check risk level distribution\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRisk Level Distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:10381\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10367\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10369\u001b[39m op = frame_apply(\n\u001b[32m  10370\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10371\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10379\u001b[39m     kwargs=kwargs,\n\u001b[32m  10380\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HealthCore\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mcalculate_risk_level\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# pH risk (WHO guidelines: 6.5-8.5)\u001b[39;00m\n\u001b[32m     27\u001b[39m ph = row[\u001b[33m'\u001b[39m\u001b[33mpH\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mph\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6.5\u001b[39;49m \u001b[38;5;129;01mor\u001b[39;00m ph > \u001b[32m8.5\u001b[39m:\n\u001b[32m     29\u001b[39m     risk_score += \u001b[32m2\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ph < \u001b[32m7.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ph > \u001b[32m8.0\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# Create Risk Level Target Variable and Prepare Features\n",
    "print(\"Creating Risk Level target variable...\")\n",
    "\n",
    "# Create a working copy of the data\n",
    "model_data = integrated_df.copy()\n",
    "\n",
    "# Remove rows with missing pH values\n",
    "model_data = model_data.dropna(subset=['pH'])\n",
    "\n",
    "# Define input features (since temperature is not available, we'll use water quality indicators)\n",
    "input_features = ['pH', 'Total_Cases']\n",
    "\n",
    "# Add additional water quality features that might correlate with health risks\n",
    "additional_features = ['TDS', 'F', 'NO3', 'Cl', 'EC in μS/cm']\n",
    "for feature in additional_features:\n",
    "    if feature in model_data.columns and model_data[feature].notna().sum() > 1000:\n",
    "        input_features.append(feature)\n",
    "        print(f\"Added {feature} to input features\")\n",
    "\n",
    "print(f\"Final input features: {input_features}\")\n",
    "\n",
    "# Create Risk Level based on multiple factors\n",
    "def calculate_risk_level(row):\n",
    "    risk_score = 0\n",
    "    \n",
    "    # pH risk (WHO guidelines: 6.5-8.5)\n",
    "    ph = row['pH']\n",
    "    if ph < 6.5 or ph > 8.5:\n",
    "        risk_score += 2\n",
    "    elif ph < 7.0 or ph > 8.0:\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Cases risk\n",
    "    cases = row['Total_Cases']\n",
    "    if cases > 10:\n",
    "        risk_score += 3\n",
    "    elif cases > 5:\n",
    "        risk_score += 2\n",
    "    elif cases > 0:\n",
    "        risk_score += 1\n",
    "    \n",
    "    # TDS risk (WHO guideline: <1000 mg/L)\n",
    "    if 'TDS' in row and pd.notna(row['TDS']):\n",
    "        tds = row['TDS']\n",
    "        if tds > 1000:\n",
    "            risk_score += 2\n",
    "        elif tds > 500:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Fluoride risk (WHO guideline: 0.5-1.5 mg/L)\n",
    "    if 'F' in row and pd.notna(row['F']):\n",
    "        fluoride = row['F']\n",
    "        if fluoride > 1.5:\n",
    "            risk_score += 2\n",
    "        elif fluoride < 0.5:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Nitrate risk (WHO guideline: <50 mg/L)\n",
    "    if 'NO3' in row and pd.notna(row['NO3']):\n",
    "        nitrate = row['NO3']\n",
    "        if nitrate > 50:\n",
    "            risk_score += 2\n",
    "        elif nitrate > 25:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Convert risk score to categorical risk level\n",
    "    if risk_score >= 6:\n",
    "        return 'High Risk'\n",
    "    elif risk_score >= 3:\n",
    "        return 'Medium Risk'\n",
    "    elif risk_score >= 1:\n",
    "        return 'Low Risk'\n",
    "    else:\n",
    "        return 'No Risk'\n",
    "\n",
    "# Apply risk calculation\n",
    "print(\"Calculating risk levels...\")\n",
    "model_data['Risk_Level'] = model_data.apply(calculate_risk_level, axis=1)\n",
    "\n",
    "# Check risk level distribution\n",
    "print(f\"\\nRisk Level Distribution:\")\n",
    "risk_counts = model_data['Risk_Level'].value_counts()\n",
    "print(risk_counts)\n",
    "print(f\"\\nRisk Level Percentages:\")\n",
    "print((risk_counts / len(model_data) * 100).round(2))\n",
    "\n",
    "print(f\"\\nModel data shape after preprocessing: {model_data.shape}\")\n",
    "print(f\"Features selected for training: {input_features}\")\n",
    "print(f\"Target variable: Risk_Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ec2fc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing data types and creating Risk Level target variable...\n",
      "Converted pH to numeric, non-null count: 49025\n",
      "Converted Total_Cases to numeric, non-null count: 49036\n",
      "Converted TDS to numeric, non-null count: 30954\n",
      "Converted F to numeric, non-null count: 30890\n",
      "Converted NO3 to numeric, non-null count: 43289\n",
      "Converted Cl to numeric, non-null count: 49027\n",
      "Converted EC in μS/cm to numeric, non-null count: 48936\n",
      "After removing pH nulls, dataset shape: (49025, 33)\n",
      "Added TDS to input features (non-null count: 30951)\n",
      "Added F to input features (non-null count: 30883)\n",
      "Added NO3 to input features (non-null count: 43282)\n",
      "Added Cl to input features (non-null count: 49020)\n",
      "Added EC in μS/cm to input features (non-null count: 48930)\n",
      "Final input features: ['pH', 'Total_Cases', 'TDS', 'F', 'NO3', 'Cl', 'EC in μS/cm']\n",
      "Calculating risk levels...\n",
      "\n",
      "Risk Level Distribution:\n",
      "Risk_Level\n",
      "Medium Risk    28246\n",
      "Low Risk       14918\n",
      "High Risk       4770\n",
      "No Risk         1091\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Risk Level Percentages:\n",
      "Risk_Level\n",
      "Medium Risk    57.62\n",
      "Low Risk       30.43\n",
      "High Risk       9.73\n",
      "No Risk         2.23\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Model data shape after preprocessing: (49025, 34)\n",
      "Features selected for training: ['pH', 'Total_Cases', 'TDS', 'F', 'NO3', 'Cl', 'EC in μS/cm']\n",
      "Target variable: Risk_Level\n"
     ]
    }
   ],
   "source": [
    "# Fix data types and create Risk Level Target Variable\n",
    "print(\"Fixing data types and creating Risk Level target variable...\")\n",
    "\n",
    "# Create a working copy of the data\n",
    "model_data = integrated_df.copy()\n",
    "\n",
    "# Convert numeric columns to proper data types\n",
    "numeric_columns = ['pH', 'Total_Cases', 'TDS', 'F', 'NO3', 'Cl', 'EC in μS/cm']\n",
    "for col in numeric_columns:\n",
    "    if col in model_data.columns:\n",
    "        model_data[col] = pd.to_numeric(model_data[col], errors='coerce')\n",
    "        print(f\"Converted {col} to numeric, non-null count: {model_data[col].notna().sum()}\")\n",
    "\n",
    "# Remove rows with missing pH values (our main feature)\n",
    "model_data = model_data.dropna(subset=['pH'])\n",
    "print(f\"After removing pH nulls, dataset shape: {model_data.shape}\")\n",
    "\n",
    "# Define input features\n",
    "input_features = ['pH', 'Total_Cases']\n",
    "\n",
    "# Add additional water quality features that have sufficient data\n",
    "additional_features = ['TDS', 'F', 'NO3', 'Cl', 'EC in μS/cm']\n",
    "for feature in additional_features:\n",
    "    if feature in model_data.columns and model_data[feature].notna().sum() > 1000:\n",
    "        input_features.append(feature)\n",
    "        print(f\"Added {feature} to input features (non-null count: {model_data[feature].notna().sum()})\")\n",
    "\n",
    "print(f\"Final input features: {input_features}\")\n",
    "\n",
    "# Create Risk Level based on multiple factors\n",
    "def calculate_risk_level(row):\n",
    "    risk_score = 0\n",
    "    \n",
    "    # pH risk (WHO guidelines: 6.5-8.5)\n",
    "    ph = row['pH']\n",
    "    if pd.notna(ph):\n",
    "        if ph < 6.5 or ph > 8.5:\n",
    "            risk_score += 2\n",
    "        elif ph < 7.0 or ph > 8.0:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Cases risk\n",
    "    cases = row['Total_Cases']\n",
    "    if pd.notna(cases):\n",
    "        if cases > 10:\n",
    "            risk_score += 3\n",
    "        elif cases > 5:\n",
    "            risk_score += 2\n",
    "        elif cases > 0:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # TDS risk (WHO guideline: <1000 mg/L)\n",
    "    if 'TDS' in row.index and pd.notna(row['TDS']):\n",
    "        tds = row['TDS']\n",
    "        if tds > 1000:\n",
    "            risk_score += 2\n",
    "        elif tds > 500:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Fluoride risk (WHO guideline: 0.5-1.5 mg/L)\n",
    "    if 'F' in row.index and pd.notna(row['F']):\n",
    "        fluoride = row['F']\n",
    "        if fluoride > 1.5:\n",
    "            risk_score += 2\n",
    "        elif fluoride < 0.5:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Nitrate risk (WHO guideline: <50 mg/L)\n",
    "    if 'NO3' in row.index and pd.notna(row['NO3']):\n",
    "        nitrate = row['NO3']\n",
    "        if nitrate > 50:\n",
    "            risk_score += 2\n",
    "        elif nitrate > 25:\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Convert risk score to categorical risk level\n",
    "    if risk_score >= 6:\n",
    "        return 'High Risk'\n",
    "    elif risk_score >= 3:\n",
    "        return 'Medium Risk'\n",
    "    elif risk_score >= 1:\n",
    "        return 'Low Risk'\n",
    "    else:\n",
    "        return 'No Risk'\n",
    "\n",
    "# Apply risk calculation\n",
    "print(\"Calculating risk levels...\")\n",
    "model_data['Risk_Level'] = model_data.apply(calculate_risk_level, axis=1)\n",
    "\n",
    "# Check risk level distribution\n",
    "print(f\"\\nRisk Level Distribution:\")\n",
    "risk_counts = model_data['Risk_Level'].value_counts()\n",
    "print(risk_counts)\n",
    "print(f\"\\nRisk Level Percentages:\")\n",
    "print((risk_counts / len(model_data) * 100).round(2))\n",
    "\n",
    "print(f\"\\nModel data shape after preprocessing: {model_data.shape}\")\n",
    "print(f\"Features selected for training: {input_features}\")\n",
    "print(f\"Target variable: Risk_Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae7d24bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for model training...\n",
      "Complete cases available: 25613 out of 49025\n",
      "Feature matrix shape: (25613, 7)\n",
      "Target vector shape: (25613,)\n",
      "\n",
      "Feature Statistics:\n",
      "                 pH   Total_Cases           TDS             F           NO3  \\\n",
      "count  25613.000000  25613.000000  25613.000000  25613.000000  25613.000000   \n",
      "mean      33.618715     26.183188    608.585443      1.209749     27.196208   \n",
      "std      178.868926     67.823094    797.351017      4.726429     48.903654   \n",
      "min        3.720000      0.000000      8.750000     -0.160000      0.000000   \n",
      "25%        7.450000      0.000000    219.000000      0.120000      3.000000   \n",
      "50%        7.750000      9.000000    414.000000      0.350000     13.000000   \n",
      "75%        8.040000     26.000000    707.000000      0.710000     35.000000   \n",
      "max     3725.000000   1268.000000  17768.000000     67.000000   1150.000000   \n",
      "\n",
      "                 Cl   EC in μS/cm  \n",
      "count  25613.000000  25613.000000  \n",
      "mean     130.506333    953.566152  \n",
      "std      306.398129   1266.915454  \n",
      "min        0.000000      0.000000  \n",
      "25%       22.000000    328.000000  \n",
      "50%       50.000000    643.000000  \n",
      "75%      119.000000   1113.000000  \n",
      "max     9217.000000  26520.000000  \n",
      "\n",
      "Data Split:\n",
      "Training set: 20490 samples\n",
      "Testing set: 5123 samples\n",
      "\n",
      "Training set class distribution:\n",
      "Risk_Level\n",
      "Medium Risk    0.594\n",
      "Low Risk       0.207\n",
      "High Risk      0.174\n",
      "No Risk        0.025\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set class distribution:\n",
      "Risk_Level\n",
      "Medium Risk    0.594\n",
      "Low Risk       0.207\n",
      "High Risk      0.174\n",
      "No Risk        0.025\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "✓ Data preprocessing completed successfully!\n",
      "✓ Features scaled using StandardScaler\n",
      "✓ Ready for model training\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for model training\n",
    "print(\"Preparing data for model training...\")\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "# First, let's only use complete cases (rows with no missing values in selected features)\n",
    "complete_data = model_data[input_features + ['Risk_Level']].dropna()\n",
    "print(f\"Complete cases available: {len(complete_data)} out of {len(model_data)}\")\n",
    "\n",
    "X = complete_data[input_features]\n",
    "y = complete_data['Risk_Level']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Check feature statistics\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution in training and test sets\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n✓ Data preprocessing completed successfully!\")\n",
    "print(f\"✓ Features scaled using StandardScaler\")\n",
    "print(f\"✓ Ready for model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b2f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multiple models for risk level prediction...\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Accuracy: 0.9922\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.9856\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6303\n",
      "\n",
      "==================================================\n",
      "BEST MODEL: Random Forest\n",
      "Best Accuracy: 0.9922\n",
      "==================================================\n",
      "\n",
      "Detailed Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   High Risk       0.99      0.99      0.99       893\n",
      "    Low Risk       0.99      0.99      0.99      1059\n",
      " Medium Risk       0.99      0.99      0.99      3044\n",
      "     No Risk       0.99      0.99      0.99       127\n",
      "\n",
      "    accuracy                           0.99      5123\n",
      "   macro avg       0.99      0.99      0.99      5123\n",
      "weighted avg       0.99      0.99      0.99      5123\n",
      "\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "[[ 882    0   11    0]\n",
      " [   0 1050    8    1]\n",
      " [   7   12 3025    0]\n",
      " [   0    1    0  126]]\n"
     ]
    }
   ],
   "source": [
    "# Train Multiple Models for Risk Level Prediction\n",
    "print(\"Training multiple models for risk level prediction...\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    if name == 'Logistic Regression':\n",
    "        # Use scaled features for logistic regression\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "    else:\n",
    "        # Tree-based models can use original features\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_accuracy = model_results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Detailed evaluation of the best model\n",
    "print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
    "print(classification_report(y_test, model_results[best_model_name]['predictions']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\nConfusion Matrix for {best_model_name}:\")\n",
    "cm = confusion_matrix(y_test, model_results[best_model_name]['predictions'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc55c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing feature importance and model interpretation...\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "       feature  importance\n",
      "1  Total_Cases    0.300960\n",
      "3            F    0.213799\n",
      "0           pH    0.157981\n",
      "4          NO3    0.128059\n",
      "2          TDS    0.090870\n",
      "6  EC in μS/cm    0.080056\n",
      "5           Cl    0.028275\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing prediction function with sample cases:\n",
      "\n",
      "Test 1 - Normal pH, No cases:\n",
      "Prediction: Low Risk\n",
      "Confidence: 1.000\n",
      "\n",
      "Test 2 - Normal pH, High cases:\n",
      "Prediction: Medium Risk\n",
      "Confidence: 0.990\n",
      "\n",
      "Test 3 - Extreme pH, Some cases:\n",
      "Prediction: Medium Risk\n",
      "Confidence: 0.920\n",
      "\n",
      "✅ Model training and evaluation completed successfully!\n",
      "✅ Best model achieved 99.22% accuracy\n",
      "✅ Model is ready for deployment and predictions\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance Analysis and Model Interpretation\n",
    "print(\"Analyzing feature importance and model interpretation...\")\n",
    "\n",
    "# Feature importance for Random Forest (best model)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': input_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nFeature Importance (Random Forest):\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "    plt.title('Feature Importance - Health Risk Prediction Model')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a prediction function\n",
    "def predict_risk_level(ph, total_cases, tds=None, fluoride=None, nitrate=None, chloride=None, ec=None):\n",
    "    \"\"\"\n",
    "    Predict health risk level based on input parameters\n",
    "    \n",
    "    Parameters:\n",
    "    - ph: pH level (required)\n",
    "    - total_cases: Number of total cases (required)  \n",
    "    - tds: Total Dissolved Solids (optional)\n",
    "    - fluoride: Fluoride content (optional)\n",
    "    - nitrate: Nitrate content (optional)\n",
    "    - chloride: Chloride content (optional)\n",
    "    - ec: Electrical Conductivity (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create input array\n",
    "    input_data = [ph, total_cases]\n",
    "    \n",
    "    # Add optional parameters (use median values if not provided)\n",
    "    if tds is not None:\n",
    "        input_data.append(tds)\n",
    "    else:\n",
    "        input_data.append(X['TDS'].median())\n",
    "        \n",
    "    if fluoride is not None:\n",
    "        input_data.append(fluoride)\n",
    "    else:\n",
    "        input_data.append(X['F'].median())\n",
    "        \n",
    "    if nitrate is not None:\n",
    "        input_data.append(nitrate)\n",
    "    else:\n",
    "        input_data.append(X['NO3'].median())\n",
    "        \n",
    "    if chloride is not None:\n",
    "        input_data.append(chloride)\n",
    "    else:\n",
    "        input_data.append(X['Cl'].median())\n",
    "        \n",
    "    if ec is not None:\n",
    "        input_data.append(ec)\n",
    "    else:\n",
    "        input_data.append(X['EC in μS/cm'].median())\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    input_array = np.array(input_data).reshape(1, -1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(input_array)[0]\n",
    "    probability = best_model.predict_proba(input_array)[0]\n",
    "    \n",
    "    # Get class probabilities\n",
    "    classes = best_model.classes_\n",
    "    prob_dict = dict(zip(classes, probability))\n",
    "    \n",
    "    return {\n",
    "        'predicted_risk_level': prediction,\n",
    "        'probabilities': prob_dict,\n",
    "        'confidence': max(probability)\n",
    "    }\n",
    "\n",
    "# Test the prediction function with sample data\n",
    "print(f\"\\nTesting prediction function with sample cases:\")\n",
    "\n",
    "# Test case 1: Normal conditions\n",
    "test1 = predict_risk_level(ph=7.2, total_cases=0)\n",
    "print(f\"\\nTest 1 - Normal pH, No cases:\")\n",
    "print(f\"Prediction: {test1['predicted_risk_level']}\")\n",
    "print(f\"Confidence: {test1['confidence']:.3f}\")\n",
    "\n",
    "# Test case 2: High cases\n",
    "test2 = predict_risk_level(ph=7.0, total_cases=50)\n",
    "print(f\"\\nTest 2 - Normal pH, High cases:\")\n",
    "print(f\"Prediction: {test2['predicted_risk_level']}\")\n",
    "print(f\"Confidence: {test2['confidence']:.3f}\")\n",
    "\n",
    "# Test case 3: Extreme pH\n",
    "test3 = predict_risk_level(ph=10.5, total_cases=5)\n",
    "print(f\"\\nTest 3 - Extreme pH, Some cases:\")\n",
    "print(f\"Prediction: {test3['predicted_risk_level']}\")\n",
    "print(f\"Confidence: {test3['confidence']:.3f}\")\n",
    "\n",
    "print(f\"\\n✅ Model training and evaluation completed successfully!\")\n",
    "print(f\"✅ Best model achieved {best_accuracy:.2%} accuracy\")\n",
    "print(f\"✅ Model is ready for deployment and predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the trained model and creating final summary...\n",
      "✓ Model saved as 'health_risk_prediction_model.pkl'\n",
      "\n",
      "HEALTH RISK PREDICTION MODEL - TRAINING SUMMARY\n",
      "===============================================\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "- Best Model: Random Forest\n",
      "- Accuracy: 0.9922 (99.22%)\n",
      "- Training Samples: 20490\n",
      "- Test Samples: 5123\n",
      "\n",
      "INPUT FEATURES (in order of importance):\n",
      "1. Total_Cases (30.1%) - Most important predictor\n",
      "2. Fluoride (21.4%) - Water quality indicator  \n",
      "3. pH (15.8%) - Water acidity/alkalinity\n",
      "4. Nitrate (12.8%) - Water contamination indicator\n",
      "5. TDS (9.1%) - Total dissolved solids\n",
      "6. Electrical Conductivity (8.0%) - Water quality measure\n",
      "7. Chloride (2.8%) - Water chemistry indicator\n",
      "\n",
      "TARGET CLASSES:\n",
      "- High Risk: 4463 samples (17.4%)\n",
      "- Medium Risk: 15221 samples (59.4%)  \n",
      "- Low Risk: 5296 samples (20.7%)\n",
      "- No Risk: 633 samples (2.5%)\n",
      "\n",
      "RISK LEVEL DEFINITION:\n",
      "Risk levels are calculated based on WHO guidelines for:\n",
      "- pH levels (optimal: 6.5-8.5)\n",
      "- Total disease cases in the area\n",
      "- Water quality parameters (TDS, Fluoride, Nitrate)\n",
      "\n",
      "USAGE EXAMPLE:\n",
      "To predict risk for new data:\n",
      "result = predict_risk_level(ph=7.2, total_cases=5, tds=500, fluoride=0.8)\n",
      "print(result['predicted_risk_level'])  # Returns: 'Low Risk', 'Medium Risk', etc.\n",
      "\n",
      "MODEL FILES SAVED:\n",
      "- health_risk_prediction_model.pkl - Complete model package\n",
      "- integrated_water_health_dataset.csv - Training dataset\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Deploy model for real-time predictions\n",
      "2. Monitor model performance over time\n",
      "3. Retrain with new data as available\n",
      "4. Integrate with health monitoring systems\n",
      "\n",
      "Testing saved model...\n",
      "Test prediction with loaded model: Medium Risk\n",
      "\n",
      "🎉 MODEL TRAINING COMPLETED SUCCESSFULLY! 🎉\n",
      "📊 Model Accuracy: 99.22%\n",
      "💾 Model saved as: health_risk_prediction_model.pkl\n",
      "🔮 Ready for health risk predictions!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save the trained model and create final summary\n",
    "import pickle\n",
    "\n",
    "print(\"Saving the trained model and creating final summary...\")\n",
    "\n",
    "# Save the best model\n",
    "model_filename = 'health_risk_prediction_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': input_features,\n",
    "        'classes': best_model.classes_,\n",
    "        'accuracy': best_accuracy\n",
    "    }, f)\n",
    "\n",
    "print(f\"✓ Model saved as '{model_filename}'\")\n",
    "\n",
    "# Create comprehensive summary\n",
    "final_summary = f\"\"\"\n",
    "HEALTH RISK PREDICTION MODEL - TRAINING SUMMARY\n",
    "===============================================\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "- Best Model: {best_model_name}\n",
    "- Accuracy: {best_accuracy:.4f} ({best_accuracy:.2%})\n",
    "- Training Samples: {len(X_train)}\n",
    "- Test Samples: {len(X_test)}\n",
    "\n",
    "INPUT FEATURES (in order of importance):\n",
    "1. Total_Cases (30.1%) - Most important predictor\n",
    "2. Fluoride (21.4%) - Water quality indicator  \n",
    "3. pH (15.8%) - Water acidity/alkalinity\n",
    "4. Nitrate (12.8%) - Water contamination indicator\n",
    "5. TDS (9.1%) - Total dissolved solids\n",
    "6. Electrical Conductivity (8.0%) - Water quality measure\n",
    "7. Chloride (2.8%) - Water chemistry indicator\n",
    "\n",
    "TARGET CLASSES:\n",
    "- High Risk: {(y == 'High Risk').sum()} samples ({(y == 'High Risk').mean():.1%})\n",
    "- Medium Risk: {(y == 'Medium Risk').sum()} samples ({(y == 'Medium Risk').mean():.1%})  \n",
    "- Low Risk: {(y == 'Low Risk').sum()} samples ({(y == 'Low Risk').mean():.1%})\n",
    "- No Risk: {(y == 'No Risk').sum()} samples ({(y == 'No Risk').mean():.1%})\n",
    "\n",
    "RISK LEVEL DEFINITION:\n",
    "Risk levels are calculated based on WHO guidelines for:\n",
    "- pH levels (optimal: 6.5-8.5)\n",
    "- Total disease cases in the area\n",
    "- Water quality parameters (TDS, Fluoride, Nitrate)\n",
    "\n",
    "USAGE EXAMPLE:\n",
    "To predict risk for new data:\n",
    "result = predict_risk_level(ph=7.2, total_cases=5, tds=500, fluoride=0.8)\n",
    "print(result['predicted_risk_level'])  # Returns: 'Low Risk', 'Medium Risk', etc.\n",
    "\n",
    "MODEL FILES SAVED:\n",
    "- {model_filename} - Complete model package\n",
    "- integrated_water_health_dataset.csv - Training dataset\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Deploy model for real-time predictions\n",
    "2. Monitor model performance over time\n",
    "3. Retrain with new data as available\n",
    "4. Integrate with health monitoring systems\n",
    "\"\"\"\n",
    "\n",
    "print(final_summary)\n",
    "\n",
    "# Test the saved model by loading it back\n",
    "print(\"Testing saved model...\")\n",
    "with open(model_filename, 'rb') as f:\n",
    "    loaded_model_package = pickle.load(f)\n",
    "\n",
    "loaded_model = loaded_model_package['model']\n",
    "test_prediction = loaded_model.predict([[7.2, 10, 500, 0.8, 20, 50, 800]])[0]\n",
    "print(f\"Test prediction with loaded model: {test_prediction}\")\n",
    "\n",
    "print(f\"\\n🎉 MODEL TRAINING COMPLETED SUCCESSFULLY! 🎉\")\n",
    "print(f\"📊 Model Accuracy: {best_accuracy:.2%}\")\n",
    "print(f\"💾 Model saved as: {model_filename}\")\n",
    "print(f\"🔮 Ready for health risk predictions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
